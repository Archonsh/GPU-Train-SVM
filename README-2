GPU-Train-SVM is a modification of GPUSVM, version 0.2.
GPUSVM, version 0.2 is an implementation of Support Vector
Machine training and classification using Nvidia Graphics
Processors.

GPU-Train-SVM is previously modified, such that GPU-Train-SVM
can train multiple SVMs concurrently.

IMPROVEMENT
    * The CUDA code has been optimized for better parallel computation
    * The memory communication between host and device is optimized
    * Code has been commented
    * Bug fix

NEW FEATURES

    * Feature importance ranking by random split feature into 2 buckets and train to compare cross-validation accuracy

PREREQUISITES

    * NVIDIA Graphics card with CUDA support
    * Latest NVIDIA drivers for GPU
    * CUDA toolkit & GPU Computing SDK 9.0 or upper

INSTRUCTIONS
    Follow the previous 'README', only difference is CUDA Version 9.0

USAGES
    Follow the previous 'README'

NEW USAGES
    The '-x 2' for cross validation has been modified.
    Now '-x 2' aims to take in a SINGLE dataset, and output the IMPORTANCE RANKING of features
    The FIRSTSEED in framework.h specify the random seed for the split process
    It will be used to generate some SECONDSEEDs for individual random permutation

PROCESS
    The data set will first be first 5-folder(by default) split to training and test set for cross-validation
    Then each folder will be further randomly split to 'num of dimension * 2' smaller SVMs
    The importance of a feature is computed by 'impt = sum(accuracy of SVMs the features is in) / sum(accuracy of SVMs the features is out)'
    Refer to Yixuan's FYP paper, algorithm 2, for more detailed information

KNOWN ISSUES
    * In some cases, the memory deallocation after the program ends will fail (No influence on the results, OS will do housekeeping)
    * In some rare cases, the CUDA driver cannot fetch the device information (No influence on the results, just try one more time)
    * In the first run after compiling, there might be Segmentation fault (core dumped) (No influence on the results, just try one more time)

FURTHER WORK TO CONSIDER
    * The current version only supports 2-class training and prediction
        * Can modify to 1-vs-1 by running multiple times
        * Can modify to 1-vs-all by pre-processing the input data (modify the labels)

    * The split feature data storage can be reorgnized to reduce memory requirement
        * Now the small SVMs possess proprietary params and data, it can be modified such that the original data is shared
        * But please consider the data access time on GPU memory, as the split will result in random scattered access
        * Scattered memory access performance on GPU global memory is VERY BAD

NOTE
    * Read the original paper for basic understanding of
        * the naming of the variables
        * what the second order phase 1-4 are doing
        * what parameters and limitations applied
        * https://www2.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-11.pdf

    * The input data format is different from those provided on LIBSVM website
        * this code requires 'label feature#:value feature#:value feature#:value feature#:value ...'
        * feature with value 0 must also be presented
        * use the python parser named format_convert.py
        * arguments are name of data files need to convert
        * example: 'python3 format_convert.py a1a.svm a2a.svm'

    * The real makefile is ./src/common/common.mk



